#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from modules.web_scraper import create_scraper_from_config


def example_single_site_scraping():
    """–ü—Ä–∏–º–µ—Ä —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
    print("üîç –ü—Ä–∏–º–µ—Ä —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∞–ø–µ—Ä
    scraper = create_scraper_from_config()
    
    # –°–∫—Ä–∞–ø–∏–º –Ω–µ–±–æ–ª—å—à–æ–π —Å–∞–π—Ç
    result = scraper.scrape_and_add(
        start_url="https://www.garant.ru/",
        max_pages=5
    )
    
    if result['success']:
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {result['pages_scraped']} —Å—Ç—Ä–∞–Ω–∏—Ü")
        print(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–æ {result['chunks_added']} —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result['message']}")


def example_multiple_sites_scraping():
    """–ü—Ä–∏–º–µ—Ä —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
    print("\nüåê –ü—Ä–∏–º–µ—Ä —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤")
    
    # –°–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
    sites = [
        "https://www.garant.ru/",
        "https://www.consultant.ru/",
        "https://www.pravo.gov.ru/"
    ]
    
    scraper = create_scraper_from_config()
    total_pages = 0
    total_chunks = 0
    
    for i, site in enumerate(sites, 1):
        print(f"\nüìã –°–∞–π—Ç {i}/{len(sites)}: {site}")
        
        result = scraper.scrape_and_add(site, max_pages=3)
        
        if result['success']:
            total_pages += result['pages_scraped']
            total_chunks += result['chunks_added']
            print(f"‚úÖ {result['pages_scraped']} —Å—Ç—Ä–∞–Ω–∏—Ü, {result['chunks_added']} —á–∞–Ω–∫–æ–≤")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['message']}")
    
    print(f"\nüéâ –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    print(f"üìù –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")


def example_custom_scraping():
    """–ü—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞"""
    print("\n‚öôÔ∏è –ü—Ä–∏–º–µ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞")
    
    from modules.knowledge_base import KnowledgeBase
    from modules.text_processing import TextProcessor
    from modules.web_scraper import WebScraper
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    knowledge_base = KnowledgeBase()
    text_processor = TextProcessor()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∞–ø–µ—Ä —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    scraper = WebScraper(knowledge_base, text_processor)
    scraper.max_pages = 10  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    scraper.delay = 2       # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
    
    # –°–∫—Ä–∞–ø–∏–º —Å–∞–π—Ç
    result = scraper.scrape_and_add(
        start_url="https://www.law.ru/",
        max_pages=5
    )
    
    if result['success']:
        print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
        print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {result['pages_scraped']}")
        print(f"üìù –ß–∞–Ω–∫–æ–≤: {result['chunks_added']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result['message']}")


def example_check_knowledge_base():
    """–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ—Å–ª–µ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""
    print("\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    
    from modules.knowledge_base import get_knowledge_base
    
    kb = get_knowledge_base()
    stats = kb.get_collection_stats()
    
    print(f"üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {stats.get('total_documents', 0)}")
    print(f"üóÇÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è: {stats.get('collection_name', 'N/A')}")
    print(f"üíæ –ü—É—Ç—å –∫ –ë–î: {stats.get('db_path', 'N/A')}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
    print("üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–±-—Å–∫—Ä–∞–ø–µ—Ä–∞")
    print("=" * 50)
    
    try:
        # –ü—Ä–∏–º–µ—Ä 1: –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞
        example_single_site_scraping()
        
        # –ü—Ä–∏–º–µ—Ä 2: –°–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
        example_multiple_sites_scraping()
        
        # –ü—Ä–∏–º–µ—Ä 3: –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä
        example_custom_scraping()
        
        # –ü—Ä–∏–º–µ—Ä 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        example_check_knowledge_base()
        
        print("\n‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
        print("   - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        print("   - –ù–∞—Å—Ç—Ä–æ–µ–Ω —Ñ–∞–π–ª .env")
        print("   - –ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É")


if __name__ == "__main__":
    main() 